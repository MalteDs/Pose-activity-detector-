{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "519efad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04a1bec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Landmarks extraídos: (195, 101)\n"
     ]
    }
   ],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "def extract_landmarks_from_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5)\n",
    "\n",
    "    landmark_rows = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Procesar frame con MediaPipe\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(frame_rgb)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            row = {}\n",
    "            for i, lm in enumerate(landmarks):\n",
    "                row[f'x{i}'] = lm.x\n",
    "                row[f'y{i}'] = lm.y\n",
    "                row[f'v{i}'] = lm.visibility\n",
    "            # Otras columnas necesarias (puedes asignar una clase dummy si no la tienes)\n",
    "            row['class_name'] = 'unknown'\n",
    "            row['label'] = -1\n",
    "            landmark_rows.append(row)\n",
    "\n",
    "    cap.release()\n",
    "    pose.close()\n",
    "\n",
    "    # Convertir a DataFrame\n",
    "    df_landmarks = pd.DataFrame(landmark_rows)\n",
    "    return df_landmarks\n",
    "\n",
    "# Ejemplo\n",
    "video_path = \"../data/validation/video5.mp4\"\n",
    "df_new_video = extract_landmarks_from_video(video_path)\n",
    "print(\"✅ Landmarks extraídos:\", df_new_video.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5616a927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset limpio de landmarks: (195, 102)\n"
     ]
    }
   ],
   "source": [
    "# Calcular visibilidad promedio\n",
    "vis_cols = [f'v{i}' for i in range(33)]\n",
    "df_new_video['v_mean'] = df_new_video[vis_cols].mean(axis=1)\n",
    "\n",
    "# Filtrar por visibilidad\n",
    "df_new_video = df_new_video[df_new_video['v_mean'] >= 0.3]\n",
    "\n",
    "# Filtrar coordenadas fuera de rango\n",
    "coord_cols = [col for col in df_new_video.columns if col.startswith(('x', 'y'))]\n",
    "df_new_video = df_new_video[(df_new_video[coord_cols] >= 0.0).all(axis=1) & (df_new_video[coord_cols] <= 1.0).all(axis=1)]\n",
    "\n",
    "print(\"✅ Dataset limpio de landmarks:\", df_new_video.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e586d118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Features generados: (195, 7)\n"
     ]
    }
   ],
   "source": [
    "def calc_angle(a, b, c):\n",
    "    \"\"\"\n",
    "    Calcula el ángulo entre tres puntos: a (proximal), b (vértice), c (distal)\n",
    "    Devuelve el ángulo en grados.\n",
    "    \"\"\"\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    c = np.array(c)\n",
    "\n",
    "    ba = a - b\n",
    "    bc = c - b\n",
    "\n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc) + 1e-6)\n",
    "    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))\n",
    "    return np.degrees(angle)\n",
    "\n",
    "\n",
    "def extract_features_from_row(row):\n",
    "    features = {}\n",
    "\n",
    "    hip_left = [row['x23'], row['y23']]\n",
    "    knee_left = [row['x25'], row['y25']]\n",
    "    ankle_left = [row['x27'], row['y27']]\n",
    "\n",
    "    hip_right = [row['x24'], row['y24']]\n",
    "    knee_right = [row['x26'], row['y26']]\n",
    "    ankle_right = [row['x28'], row['y28']]\n",
    "\n",
    "    shoulder_left = [row['x11'], row['y11']]\n",
    "    shoulder_right = [row['x12'], row['y12']]\n",
    "\n",
    "    features['angle_knee_left'] = calc_angle(hip_left, knee_left, ankle_left)\n",
    "    features['angle_knee_right'] = calc_angle(hip_right, knee_right, ankle_right)\n",
    "    features['angle_hip_left'] = calc_angle(shoulder_left, hip_left, knee_left)\n",
    "    features['angle_hip_right'] = calc_angle(shoulder_right, hip_right, knee_right)\n",
    "\n",
    "    trunk_vector = np.array(shoulder_right) + np.array(shoulder_left) - np.array(hip_right) - np.array(hip_left)\n",
    "    features['trunk_inclination'] = np.arctan2(trunk_vector[1], trunk_vector[0]) * 180 / np.pi\n",
    "\n",
    "    features['shoulder_dist'] = np.linalg.norm(np.array(shoulder_left) - np.array(shoulder_right))\n",
    "    features['hip_dist'] = np.linalg.norm(np.array(hip_left) - np.array(hip_right))\n",
    "\n",
    "    return features\n",
    "\n",
    "# Generar features\n",
    "feature_rows = []\n",
    "for idx, row in df_new_video.iterrows():\n",
    "    feats = extract_features_from_row(row)\n",
    "    feature_rows.append(feats)\n",
    "\n",
    "df_new_features = pd.DataFrame(feature_rows)\n",
    "print(\"✅ Features generados:\", df_new_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41d88eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modelo cargado correctamente\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Etiquetas predichas: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0]\n",
      "Clase global del video: 0\n",
      "CAMINAS HACIA ADELANTE\n"
     ]
    }
   ],
   "source": [
    "# Cargar modelo ya entrenado\n",
    "model = tf.keras.models.load_model(\"./models/movimiento_classifier.h5\")\n",
    "print(\"✅ Modelo cargado correctamente\")\n",
    "\n",
    "# Predecir\n",
    "# Supongamos que tienes un nuevo DataFrame de features llamado `df_new_features`\n",
    "# y quieres hacer la predicción de clases\n",
    "\n",
    "predictions = model.predict(df_new_features)\n",
    "# La salida es un array de probabilidades por clase\n",
    "predicted_labels = predictions.argmax(axis=1)\n",
    "\n",
    "print(\"Etiquetas predichas:\", predicted_labels)\n",
    "\n",
    "predicted_class = np.bincount(predicted_labels).argmax()\n",
    "print(\"Clase global del video:\", predicted_class)\n",
    "\n",
    "if predicted_class == 0:\n",
    "    print(\"CAMINAS HACIA ADELANTE\")\n",
    "elif predicted_class == 1:\n",
    "    print(\"CAMINAS HACIA ATRÁS\")\n",
    "elif predicted_class == 2:\n",
    "    print(\"TE LEVANTAS\")\n",
    "elif predicted_class == 3:\n",
    "    print(\"TE SIENTAS\")\n",
    "elif predicted_class == 4:\n",
    "    print(\"DAS UNA VUELTA\")\n",
    "else:\n",
    "    print(\"El video no corresponde a ninguna clase conocida.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1626b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clase global del video: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76875a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(predicted_labels, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"Clase {u}: {c} frames\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
